<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Ch8.8.1: Master Theorem - C++ Tutorial</title>
  <link rel="stylesheet" href="/style.css">
  <link rel="manifest" href="/manifest.json">
  <link rel="icon" type="image/webp" href="/icons/logo.webp">
</head>
<body>
  <main>
    <h1>Ch8.8.1: Master Theorem (Appendix)</h1>

    <section>
      <h2>Overview</h2>

      <p>
        Many algorithms are defined recursively. To analyze their time complexity,
        we often write a <strong>recurrence relation</strong> describing how the
        running time depends on smaller subproblems.
      </p>

      <p>
        The <strong>Master Theorem</strong> is a tool that solves many common
        recurrences of the form:
      </p>

      <pre><code class="language-cpp">
T(n) = a · T(n / b) + f(n)
      </code></pre>

      <p>
        where:
      </p>

      <ul>
        <li><code>a</code> = number of subproblems</li>
        <li><code>b</code> = factor by which the problem size shrinks</li>
        <li><code>f(n)</code> = cost outside the recursive calls</li>
      </ul>

      <p>
        This appendix teaches you how to compute and prove time complexity using
        the Master Theorem, with examples including mergesort and quicksort.
      </p>
    </section>

    <section>
      <h2>1. The Master Theorem</h2>

      <p>
        Consider the recurrence:
      </p>

      <pre><code class="language-cpp">
T(n) = a · T(n / b) + f(n)
      </code></pre>

      <p>
        Define the <strong>critical exponent</strong>:
      </p>

      <pre><code class="language-cpp">
n^(log_b(a))
      </code></pre>

      <p>
        Compare <code>f(n)</code> to <code>n^(log_b(a))</code>.
      </p>

      <h3>Case 1: f(n) is smaller</h3>

      <pre><code class="language-cpp">
f(n) = O(n^(log_b(a) - ε))
      </code></pre>

      <p>
        for some <code>ε &gt; 0</code>.
      </p>

      <p>
        Then:
      </p>

      <pre><code class="language-cpp">
T(n) = Θ(n^(log_b(a)))
      </code></pre>

      <h3>Case 2: f(n) matches the critical exponent</h3>

      <pre><code class="language-cpp">
f(n) = Θ(n^(log_b(a)) · ln^k(n))
      </code></pre>

      <p>
        for some <code>k ≥ 0</code>.
      </p>

      <p>
        Then:
      </p>

      <pre><code class="language-cpp">
T(n) = Θ(n^(log_b(a)) · ln^(k+1)(n))
      </code></pre>

      <h3>Case 3: f(n) is larger</h3>

      <pre><code class="language-cpp">
f(n) = Ω(n^(log_b(a) + ε))
      </code></pre>

      <p>
        for some <code>ε &gt; 0</code>, and a regularity condition holds.
      </p>

      <p>
        Then:
      </p>

      <pre><code class="language-cpp">
T(n) = Θ(f(n))
      </code></pre>
    </section>

    <section>
      <h2>2. Example: Mergesort</h2>

      <p>
        Mergesort splits the array into two halves, recursively sorts each half,
        and then merges them in linear time.
      </p>

      <pre><code class="language-cpp">
T(n) = 2 · T(n / 2) + Θ(n)
      </code></pre>

      <p>
        Here:
      </p>

      <ul>
        <li><code>a = 2</code></li>
        <li><code>b = 2</code></li>
        <li><code>f(n) = Θ(n)</code></li>
      </ul>

      <p>
        Critical exponent:
      </p>

      <pre><code class="language-cpp">
n^(log_2(2)) = n
      </code></pre>

      <p>
        Since <code>f(n) = Θ(n)</code>, this is <strong>Case 2</strong>.
      </p>

      <pre><code class="language-cpp">
T(n) = Θ(n · ln(n))
      </code></pre>
    </section>

    <section>
      <h2>3. Example: Quicksort (average case)</h2>

      <p>
        In the average case, Quicksort splits the array into two subproblems of
        roughly equal size, and partitioning takes linear time.
      </p>

      <pre><code class="language-cpp">
T(n) = 2 · T(n / 2) + Θ(n)
      </code></pre>

      <p>
        This is the same recurrence as mergesort, so:
      </p>

      <pre><code class="language-cpp">
T(n) = Θ(n · ln(n))
      </code></pre>

      <p>
        This explains why Quicksort is fast on average.
      </p>
    </section>

    <section>
      <h2>4. Example: Quicksort (worst case)</h2>

      <p>
        In the worst case (pivot always smallest or largest), the recurrence becomes:
      </p>

      <pre><code class="language-cpp">
T(n) = T(n - 1) + Θ(n)
      </code></pre>

      <p>
        Expand the recurrence:
      </p>

      <pre><code class="language-cpp">
T(n) = n + (n - 1) + (n - 2) + ... + 1 = Θ(n^2)
      </code></pre>

      <p>
        This does not fit the Master Theorem, but it is easy to compute directly.
      </p>
    </section>

    <section>
      <h2>5. Example: Binary tree recursion</h2>

      <p>
        Consider a function that recursively processes a binary tree:
      </p>

      <pre><code class="language-cpp">
T(n) = 2 · T(n / 2) + Θ(1)
      </code></pre>

      <p>
        Here:
      </p>

      <ul>
        <li><code>a = 2</code></li>
        <li><code>b = 2</code></li>
        <li><code>f(n) = Θ(1)</code></li>
      </ul>

      <p>
        Critical exponent:
      </p>

      <pre><code class="language-cpp">
n^(log_2(2)) = n
      </code></pre>

      <p>
        Since <code>f(n) = Θ(1)</code> is smaller than <code>n</code>, this is
        <strong>Case 1</strong>.
      </p>

      <pre><code class="language-cpp">
T(n) = Θ(n)
      </code></pre>

      <p>
        This matches the intuition: visiting each node once is linear time.
      </p>
    </section>

    <section>
      <h2>6. When the Master Theorem does not apply</h2>

      <p>
        The Master Theorem does <strong>not</strong> apply to:
      </p>

      <ul>
        <li>recurrences where subproblem sizes are not equal</li>
        <li>recurrences like <code>T(n) = T(n - 1) + T(n - 2)</code></li>
        <li>recurrences with variable branching factors</li>
        <li>recurrences with non-polynomial <code>f(n)</code></li>
      </ul>

      <p>
        For example, the Fibonacci recurrence:
      </p>

      <pre><code class="language-cpp">
T(n) = T(n - 1) + T(n - 2) + Θ(1)
      </code></pre>

      <p>
        grows as <code>Θ(2^n)</code>, but this cannot be derived using the Master
        Theorem.
      </p>
    </section>

    <section>
      <h2>Key takeaways</h2>

      <ul>
        <li>The Master Theorem solves recurrences of the form <code>T(n) = a·T(n/b) + f(n)</code>.</li>
        <li>Compare <code>f(n)</code> to <code>n^(log_b(a))</code> to determine the case.</li>
        <li>Case 1: <code>f(n)</code> is smaller → <code>Θ(n^(log_b(a)))</code>.</li>
        <li>Case 2: <code>f(n)</code> matches the critical exponent → <code>Θ(n^(log_b(a)) · ln(n))</code>.</li>
        <li>Case 3: <code>f(n)</code> is larger → <code>Θ(f(n))</code> (with regularity conditions).</li>
        <li>Mergesort and average-case Quicksort both satisfy <code>T(n) = 2T(n/2) + Θ(n)</code> → <code>Θ(n·ln(n))</code>.</li>
        <li>Worst-case Quicksort is <code>Θ(n^2)</code> and does not fit the theorem.</li>
        <li>Not all recurrences can be solved with the Master Theorem (e.g., Fibonacci).</li>
      </ul>

      <p>
        The Master Theorem is one of the most important tools for analyzing
        divide-and-conquer algorithms. With it, you can compute the time
        complexity of many recursive algorithms quickly and rigorously.
      </p>
    </section>

    <script src="/sw-register.js"></script>
    <div class="page-navigation">
      <a href="/docs/08.algorithm/08.timecomplexity/" class="prev-button"> Ch8.8: Time &amp; Space Complexity</a>
      <a href="/" class="main-button">↑ Main Page</a>
    </div>
  </main>
</body>
</html>
